{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e059e7-bd04-41f1-b922-c9905b3aee0c",
   "metadata": {},
   "source": [
    "#### Import ML and Data Analysis Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427f6a28-6443-45f8-8817-c6eb6b339bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5c71d-08b8-4800-97f3-ecff6aacf586",
   "metadata": {},
   "source": [
    "#### Import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cbb030-5fc5-4bba-b6b0-56cf3b788a26",
   "metadata": {},
   "source": [
    "The dataset is given with no column titles. So we will use \"np.loadtxt\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9feb844d-cb68-494b-baee-4dbe59ada136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1620.  , 1620.  ,   19.73, ..., 1603.8 ,    5.  ,   92.  ],\n",
       "       [2160.  , 2160.  ,    5.33, ...,    0.  ,    0.  ,    0.  ],\n",
       "       [2160.  , 2160.  ,    5.33, ...,    0.  ,    0.  ,  388.  ],\n",
       "       ...,\n",
       "       [2160.  , 2160.  ,    6.14, ...,    0.  ,    0.  ,    0.  ],\n",
       "       [1620.  , 1620.  ,    5.33, ...,  615.6 ,    0.  ,   90.  ],\n",
       "       [1674.  , 3348.  ,    5.33, ...,    0.  ,    0.  ,    0.  ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = np.loadtxt(\"Audiobooks_data.csv\", delimiter = \",\")\n",
    "unscaled_inputs_all = raw_data[:,1:-1]\n",
    "unscaled_inputs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8835aea9-69bd-4432-8993-497df2e5ced1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14084,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_all = raw_data[:,-1]\n",
    "targets_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b60110d-585e-491f-bd80-5a8eb2c51e50",
   "metadata": {},
   "source": [
    "#### Shuffle the dataset once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0acfb-12fb-4069-82b3-abd4d4ec1576",
   "metadata": {},
   "source": [
    "We would like to shuffle the data before the balancing happens.\n",
    "\n",
    "If the original dataset was ordered in a particular way, we want to eliminate this to have least amount of bias towards that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29077f9d-6224-4d71-a1ca-9537d60a568d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14084"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_data = targets_all.shape[0]\n",
    "num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db20bd7c-8520-4bff-a3b8-b291d668f905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   40  6977  8498 ...  5158  6568 13349]\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices = np.arange(num_data)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "print(shuffled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "476bec3b-d388-4843-94db-6cc88c6697c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_inputs_all = unscaled_inputs_all[shuffled_indices]\n",
    "targets_all = targets_all[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f7d2503-e047-4830-a717-91697a4afee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 1., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e3130-3797-4dab-8e21-b85bdc7a9eb7",
   "metadata": {},
   "source": [
    "#### Balancing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b37fd-8994-4604-9a40-b5720e5a0ea8",
   "metadata": {},
   "source": [
    "There are way too many customers that did not return to buy another audiobook. \n",
    "\n",
    "We have to balance the dataset so that there is no bias towards one type of user in the model. For this, we will keep equal number of data from customers who returned and customers who didn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "009242dd-ef50-49f3-a1e9-1ea2c09e711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))\n",
    "zero_targets_counter = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "# iterating over all \"targets\"; hence, iterating over all the customer data\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            # say num_one_targets == 100. Then we should have 100 data whose target is zero. \n",
    "            # after we collected 100 data with target == 0, then zero_targets_counter == 100.\n",
    "            # if the current data has target zero, then zero_targets_counter will increase by one, going over 100.\n",
    "            # then we don't want to include this data because we already gathered enough zeros.\n",
    "            indices_to_remove.append(i)\n",
    "        \n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b25c687-3e0e-44c7-a5ff-2ba1791d9433",
   "metadata": {},
   "source": [
    "#### Standardize The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93739872-6a98-4d5a-9576-b80dce61c175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.20635076,  0.37673481, -0.38128345, ..., -0.02161991,\n",
       "        -0.19531834, -0.6015741 ],\n",
       "       [ 0.13031187, -0.23881977, -0.38128345, ..., -0.37483335,\n",
       "        -0.19531834, -0.75347362],\n",
       "       [-0.94572702, -0.85437435, -0.38128345, ..., -0.37483335,\n",
       "        -0.19531834,  0.08197375],\n",
       "       ...,\n",
       "       [ 0.13031187, -0.23881977, -0.38128345, ..., -0.37483335,\n",
       "        -0.19531834,  0.75467163],\n",
       "       [ 0.13031187, -0.23881977, -0.3218385 , ..., -0.37483335,\n",
       "        -0.19531834, -0.75347362],\n",
       "       [-0.73051925, -0.73126343, -0.38128345, ..., -0.37483335,\n",
       "        -0.19531834, -0.73177369]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors) # \"preprocessing.scale\" method standardizes the given data\n",
    "scaled_inputs # an element at index i is a row, i.e. data from one customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f21cab-8de0-4177-9d83-7f87d4837aa5",
   "metadata": {},
   "source": [
    "#### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b89ce48-1e18-457b-81bf-6ae8e64d5d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 4471, 4472, 4473])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0]) \n",
    "shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "159d29ac-2b27-4320-a5fe-5cd357478358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1161, 4205, 1993, ..., 3573, 3311, 2474])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(shuffled_indices)\n",
    "shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe84c8ff-821e-485f-bd49-2930f7adf1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_inputs = scaled_inputs[shuffled_indices] # shuffled_indices is a set of indices, so it can be used inside operator[] \n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc09bfa-e5d8-4bcc-8166-0d44c81c2770",
   "metadata": {},
   "source": [
    "#### Manage training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c92bbb60-4b17-44a0-9b0e-bd23107f9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "#80-10-10 split\n",
    "\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - (train_samples_count + validation_samples_count)\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:(train_samples_count+validation_samples_count)]\n",
    "validation_targets = shuffled_targets[train_samples_count:(train_samples_count+validation_samples_count)]\n",
    "\n",
    "test_inputs = shuffled_inputs[(train_samples_count+validation_samples_count):]\n",
    "test_targets = shuffled_targets[(train_samples_count+validation_samples_count):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64386bc3-da4b-4822-b9d8-b06917d0e6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of returned customers to all, in the training data: 0.49594858899133837\n",
      "Ratio of returned customers to all, in the validation data: 0.5078299776286354\n",
      "Ratio of returned customers to all, in the test data: 0.5245535714285714\n"
     ]
    }
   ],
   "source": [
    "print(\"Ratio of returned customers to all, in the training data:\",np.sum(train_targets)/train_samples_count)\n",
    "print(\"Ratio of returned customers to all, in the validation data:\",np.sum(validation_targets)/validation_samples_count)\n",
    "print(\"Ratio of returned customers to all, in the test data:\",np.sum(test_targets)/test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b07f9-5e95-4cab-93c6-1db6c99289f4",
   "metadata": {},
   "source": [
    "#### Save the datasets into .npz format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6d032fd-1732-4387-8f32-0a1f5664f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"audiobooks_data_train\", inputs=train_inputs,targets=train_targets)\n",
    "np.savez(\"audiobooks_data_validation\", inputs=validation_inputs,targets=validation_targets)\n",
    "np.savez(\"audiobooks_data_test\", inputs=test_inputs,targets=test_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
